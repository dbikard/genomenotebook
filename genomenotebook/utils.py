"""This contains useful functions"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/API/04_utils.ipynb.

# %% auto 0
__all__ = ['strand_dict', 'download_file', 'is_gzipped_file', 'default_open_gz', 'extract_attribute', 'extract_all_attributes',
           'extract_attributes', 'get_attributes', 'attributes_to_columns', 'set_positions', 'EmptyDataFrame',
           'parse_gff', 'available_feature_types', 'available_attributes', 'parse_fasta', 'regions_overlap',
           'add_z_order', 'get_cds_unique_name', 'get_cds_name', 'seqRecord_to_df', 'parse_recs', 'parse_genbank',
           'inspect_feature_types', 'in_wsl', 'add_extension']

# %% ../nbs/API/04_utils.ipynb 5
import numpy as np
import pandas as pd
import io

from collections import defaultdict, OrderedDict
import warnings
import gzip
import urllib.request
import os
import re
from platform import uname

from Bio import SeqIO
from Bio.Seq import Seq

from typing import List, Optional, Dict, Tuple
from IPython.display import display, HTML



# %% ../nbs/API/04_utils.ipynb 6
def download_file(url, save_path):
    """Checks if a file with the same name is already in the save_path. If not download it."""
    if os.path.exists(save_path):
        print(f"File already exists: {save_path}")
    else:
        urllib.request.urlretrieve(url, save_path)
        print(f"File downloaded and saved: {save_path}")

# %% ../nbs/API/04_utils.ipynb 7
def is_gzipped_file(file_path):
    try:
        with gzip.open(file_path, 'rb') as f:
            # Attempt to read a small chunk from the file
            f.read(1)
        return True
    except IOError:
        return False

# %% ../nbs/API/04_utils.ipynb 8
def default_open_gz(gff_path):
    """If file is gzipped then opens it with `gzip.open`, otherwise opens it with `open`"""
    if is_gzipped_file(gff_path):
        return gzip.open(gff_path,'rt')
    else:
        return open(gff_path,'r')

# %% ../nbs/API/04_utils.ipynb 10
def extract_attribute(input_str:str, #attribute string to parse
                      attr_name:str, #name of the attribute to extract
                     ) -> str:
    """Extracts the attribute called attr_name from the GFF attributes string"""
    
    pattern = f"[{attr_name[0].lower()}{attr_name[0].upper()}]{attr_name[1:]}=(?P<{attr_name}>[^;]+)"
    match = re.search(pattern, input_str)
    if match:
        return match.groupdict()[attr_name]
    else:
        return None

# %% ../nbs/API/04_utils.ipynb 13
def extract_all_attributes(input_str:str)->OrderedDict: #TODO: why is this not limited by the attributes subset provided to GenomeBrowser?
    """Extracts all attributes from the GFF attributes column"""
    
    pattern = "(?P<key>\w+[-\w]*)=(?P<value>[^;]+)"
    match = re.findall(pattern, input_str)
    d=OrderedDict()
    d.update(match)
    return d

# %% ../nbs/API/04_utils.ipynb 14
def extract_attributes(input_str:str, #the attribute string of a GFF fome
                       attributes: Optional[List[str]] = None #an optional list of attribute names to extract. If None all attributes are extracted.
                       )->OrderedDict: 
    """Extracts attributes from the GFF attributes column"""
    pattern = "(?P<key>\w+[-\w]*)=(?P<value>[^;]+)"
    match = re.findall(pattern, input_str)
    d=OrderedDict()
    if attributes is not None:
        match = [m for m in match if m[0] in attributes]
    d.update(match)
    return d

# %% ../nbs/API/04_utils.ipynb 15
def get_attributes(df: pd.DataFrame, #a features DataFrame with at least a "type" column and an "attributes_str" column
                   attributes: Optional[Dict[str, List]] = None # a dictionary with feature types as keys and a list of attributes to extract as values 
                   ) -> List:
    """Iterates over each row of the df and extracts the attributes specified in the attributes dictionary for each feature type"""
    attr_list=[]
    for i, row in df.iterrows():
        if attributes is None:
            attrs = None
        elif row.type in attributes:
            attrs = attributes[row.type]
        else:
            attrs = None
        
        attr_list.append(extract_attributes(row.attributes_str,attrs))

    return attr_list

# %% ../nbs/API/04_utils.ipynb 17
def attributes_to_columns(features: pd.DataFrame):
    attr_dicts=features.attributes.apply(extract_all_attributes)
    all_keys=list(set().union(*[d.keys() for d in attr_dicts]))
    
    attr_dict=dict([(k,[d.get(k,None) for d in attr_dicts]) for k in all_keys])
    features=features.copy()
    for k,v in attr_dict.items():
        features[k]=v
    
    features.fillna("")
    return features
    

# %% ../nbs/API/04_utils.ipynb 18
def set_positions(annotation: pd.DataFrame, # an annotation DataFrame extracted from a gff file
                            ) ->  pd.DataFrame:
    """Sets left and right as the position of the feature on the sequence, left is always lower than right.
    start and end represent the begining and end of the feature where start can be greater than end depending on the feature strand.
    """
    annotation=annotation.copy()
    annotation.loc[:, "left"] = annotation[["start"]].values
    annotation.loc[:, "right"] = annotation[["end"]].values
    
    mask = annotation["strand"] == "+"
    annotation.loc[mask, "start"] = annotation.loc[mask, "left"].values
    annotation.loc[mask, "end"] = annotation.loc[mask, "right"].values
    
    mask = annotation["strand"] == "-"
    annotation.loc[mask, "start"] = annotation.loc[mask, "right"].values
    annotation.loc[mask, "end"] = annotation.loc[mask, "left"].values
    
    annotation["middle"] = (annotation.right + annotation.left) / 2
    
    return annotation

# %% ../nbs/API/04_utils.ipynb 19
class EmptyDataFrame(Exception):
    pass

# %% ../nbs/API/04_utils.ipynb 20
def parse_gff(gff_path:str, # path to the gff file
              seq_id: Optional[str] = None, # sequence id (first column of the gff), if not None, then return only the annotations for the seq_id with this name
              first: bool = True, # if True then return only the annotations for the first sequence (or the first with seq_id)
              bounds: Optional[tuple] = None, # (left limit, right limit)
              feature_types: Optional[list] = None, # list of feature types to extract
              attributes: Optional[Dict[str, List]] = None, # a dictionary with feature types as keys and a list of attributes to extract as values 
             )->List[pd.DataFrame]:
    """ Parses a GFF3 file and returns a list of Pandas DataFrames with the data for a specific contig. 
    If seq_id is None then only the first contig is parsed.
    If feature_types is None then all feature types are extracted."""

    if attributes is None:
        attributes = {}

    def _slurp_buffer(file_buffer, buffer_empty):
        # Reset the file pointer to the beginning of the file buffer
        file_buffer.seek(0)
        if buffer_empty:
            raise EmptyDataFrame("The annotation DataFrame is empty. Check that the feature_types and seq_id are correct, and that bounds (if specified) fall within the size of your genome.")
            df=pd.DataFrame(columns=["seq_id", "source","type","start","end","score","strand","phase","attributes"])
        else:
            df=pd.read_csv(file_buffer,sep="\t",header=None)
            df.columns=["seq_id", "source","type","start","end","score","strand","phase","attributes_str"]
            #df=attributes_to_columns(df)
            df["attributes"] = get_attributes(df, attributes)
            df.drop(columns=["attributes_str"], inplace=True)
            df=set_positions(df)
        return df
    
    out = list()

    #NOTE: This assumes that all lines for a given seq_id are consecutive, which is generally the case for gff files.
    with default_open_gz(gff_path) as gff_file:
        # Create an in-memory file buffer using the io.StringIO class
        file_buffer = io.StringIO()
        buffer_empty = True
        last_seq_id = None
        for line in gff_file:
            if line[0]=="#":
                continue
            else:
                r=line.split('\t')
                current_line_seqid = r[0]
                if last_seq_id is not None and current_line_seqid != last_seq_id: #seeing a new segment of the gff
                    if not buffer_empty:
                        if seq_id is None or seq_id == last_seq_id:
                            out.append(_slurp_buffer(file_buffer, buffer_empty))
                            file_buffer = io.StringIO()
                            buffer_empty = True
                            if first is not None or seq_id is not None:
                                break
                last_seq_id = current_line_seqid
                if seq_id is None: #
                    seq_id = current_line_seqid
                if r[0]==seq_id:
                    if feature_types==None or r[2] in feature_types:
                        if bounds==None or (int(r[3])<bounds[1] and int(r[4])>bounds[0]):
                            # Write each line to the file buffer
                            file_buffer.write(line)
                            buffer_empty=False
        if not buffer_empty:
            if seq_id is None or seq_id == last_seq_id:
                out.append(_slurp_buffer(file_buffer, buffer_empty))
    
    if len(out) == 0:
        raise EmptyDataFrame("The annotation DataFrame is empty. Check that the feature_types and seq_id are correct, and that bounds (if specified) fall within the size of your genome.")
    return out

# %% ../nbs/API/04_utils.ipynb 29
def available_feature_types(gff_path):
    ftypes=set()
    with default_open_gz(gff_path) as handle:
        for line in handle:
            if line[0]!="#":
                r=line.split('\t')
                if len(r)==9:
                    ftypes.add(r[2])
    return ftypes

# %% ../nbs/API/04_utils.ipynb 31
def available_attributes(gff_path):
    features=parse_gff(gff_path)[0]
    return features.columns

# %% ../nbs/API/04_utils.ipynb 33
def parse_fasta(genome_path, seq_id):
    """Retrieves the Biopython SeqRecord object that matches the seq_id in a fasta file"""

    rec_found=False
    with open(genome_path,'r') as f:
        for rec in SeqIO.parse(f, 'fasta'):
            if rec.id==seq_id:
                rec_found=True
                break

    if not rec_found:
        warnings.warn("seq_id not found in fasta file")
        rec = None
    
    return rec.seq

# %% ../nbs/API/04_utils.ipynb 35
def regions_overlap(region1, region2, min_overlap_fraction=0.0):
    """
        regions are tuples of start and stop coordinates
        returns true if a fraction of region2 >= min_overlap_fraction overlaps with region1
        coordinates within regions must be sorted low to high
    """

    if min_overlap_fraction >= 1:
        return False
    
    # incoming coordinates are gff-base (1-based, inclusive), convert to python-base (0-based)

    region_1 = (region1[0] - 1, region1[1])
    region_2 = (region2[0] - 1, region2[1])
    region_1_size = region1[1] - region1[0]
    region_2_size = region2[1] - region2[0]
    if region_1_size == 0:
        region_1_size = 0.1
    if region_2_size == 0:
        region_2_size = 0.1

    # -region2 start is contained in region11
    if region2[0] <= region1[1] and region2[0] >= region1[0]:
        # not true for completely contained, but we don't care, should technically be min(region2[1] - region2[0], region1[1] - region2[0])
        overlap_size = region1[1] - region2[0]
        if (overlap_size / region_2_size) >= min_overlap_fraction:
            return True

    # -region2 end is contained in region1
    if region2[1] <= region1[1] and region2[1] >= region1[0]:
        # not true for completely contained, but we don't care
        overlap_size = region2[1] - region1[0]
        if (overlap_size / region_2_size) >= min_overlap_fraction:
            return True

    # -region1 start is contained in region2
    if region1[0] <= region2[1] and region1[0] >= region2[0]:
        # not true for completely contained, but we don't care
        overlap_size = region2[1] - region1[0]
        if (overlap_size / region_2_size) >= min_overlap_fraction:
            return True

    # -region1 end is contained in region2
    if region1[1] <= region2[1] and region1[1] >= region2[0]:
        # not true for completely contained, but we don't care
        overlap_size = region1[1] - region2[0]
        if (overlap_size / region_2_size) >= min_overlap_fraction:
            return True

    return False
    

# %% ../nbs/API/04_utils.ipynb 37
from collections import defaultdict

# %% ../nbs/API/04_utils.ipynb 38
def add_z_order(features, 
                prescedence = ["source", "CDS", "repeat_region", "ncRNA", "rRNA", "tRNA","exon"]):
    """
        features is a dataframe of features
        prescedence is a list of feature types in order of prescedence, e.g. ["CDS", "repeat_region", "ncRNA", "rRNA", "tRNA"] will put "CDS" features closer to the bottom of the plot than "repeat_region" features.
        returns features with a z_order column added
    """
    #TODO: possibility for "linking attributes" to link features and cause them to have the same z-order and occupy their entire envelope.
    type_order = defaultdict(lambda: len(prescedence)+1)
    type_order.update({t: i for i, t in enumerate(prescedence)})
    features.sort_values(by="start", inplace=True)
    features.sort_values(by="type", inplace=True, key=lambda x: x.map(type_order))
    z_order = []
    added = []
    all_z = {0}
    for index, row in features.iterrows():
        left, right = row["left"], row["right"]
        z = 0
        z_found = set()
        for (l_a, r_a, z_a, z_o) in added:
            if regions_overlap((left, right), (l_a, r_a)):
                if type_order[row["type"]] > z_o:
                    for i in range(z_a+1):
                        z_found.add(i)
                else:
                    z_found.add(z_a)
        if len(z_found) == len(all_z):
            z = max(all_z) + 1
            all_z.add(z)
        else:
            z = min(all_z - z_found)
        z_order.append(z)
        added.append((left, right, z, type_order[row["type"]]))
    features["z_order"] = z_order

    features.sort_values(by="start", inplace=True)

# %% ../nbs/API/04_utils.ipynb 40
#### Code from Domainator
def get_cds_unique_name(feature):
    """
        If the feature already has a cds_id, then keep it, otherwise generate one based on the position on the contig.
    """
    if "cds_id" in feature.qualifiers:
        return feature.qualifiers["cds_id"][0]
    else:
        # need the strand information to account for circular contigs.
        name_parts = ["_".join( (str(p.stranded_start_human_readable), str(p.strand), str(p.stranded_end_human_readable)) ) for p in feature.location.parts]
        return " ".join(name_parts) # space so it can be split into multiple lines when writing genbank files

def get_cds_name(feature): #(contig_id, feature):
    if "gene_id" in feature.qualifiers:
        return feature.qualifiers["gene_id"][0]
    elif "locus_tag" in feature.qualifiers:
        return feature.qualifiers["locus_tag"][0]
    else:
        return get_cds_unique_name(feature)
#### End code from Domainator

# %% ../nbs/API/04_utils.ipynb 41
from Bio import SeqRecord

# %% ../nbs/API/04_utils.ipynb 42
strand_dict = {1: "+", -1: "-"}

def seqRecord_to_df(rec: SeqRecord,
                    feature_types: Optional[List[str]] = None, # if None then get all features, otherwise only those with type in FeatureTypes.
                    attributes: Optional[Dict[str,List]] = None 
                    # if None, then get all attributes of all feature types. If dict, then only get attributes of feature types keys. If value is None, get all
                    )->pd.DataFrame:
                    
    feature_lists = []
    for feature in rec.features:
        if feature_types is None or feature.type in feature_types:
            if attributes is None:
                attrs = None
            else:
                attrs=attributes.get(feature.type, None)
            for part in feature.location.parts:
                attributes_list = []#[("ID", get_cds_name(feature)),]
                for key, value in feature.qualifiers.items():
                    if key == "translation":
                        continue
                    #if key == "ID":
                    #    continue
                    if (attrs==None) or (key in attrs):
                        if len(value) == 1:
                            attributes_list.append((key, value[0]))
                        else:
                            attributes_list.append((key, "; ".join(value)))

                attributes_dict = OrderedDict( attributes_list )
                feature_lists.append([rec.id, 'Genbank', feature.type, part.start+1, part.end, '.', strand_dict.get(part.strand, "."), ".", attributes_dict])
        
    df=pd.DataFrame(feature_lists, columns=["seq_id", "source", "type", "start", "end", "score", "strand", "phase", "attributes"])
    return df

# %% ../nbs/API/04_utils.ipynb 45
def parse_recs(recs, # iterator over Bio.SeqRecord.SeqRecord
                   seq_id: Optional[str] = None, # sequence id (first column of the gff), if not None, then return only the annotations for the seq_id with this name
                   first = True, # if True then return only the annotations for the first sequence (or the first with seq_id)
                   bounds: Optional[tuple] = None, # (left limit, right limit)
                   feature_types: Optional[list] = None, # list of feature types to extract
                   attributes: Optional[Dict[str, List]] = None, # a dictionary with feature types as keys and a list of attributes to extract as values 
               )->Tuple[List[Seq], List[pd.DataFrame]]:

    # read genbank file(s)
    feature_dfs = [] # list of dataframes, one for each seq record used if seq_id == "all"
    seqs = [] # list of Seqs
    for rec in recs:
        if seq_id == rec.id or seq_id is None:
            df = seqRecord_to_df(rec, feature_types=feature_types, attributes=attributes)
            if bounds is not None:
                df = df.loc[(df.end>bounds[0]) & (df.start<bounds[1])]
            
            df = set_positions(df)
            feature_dfs.append(df)
            seqs.append(rec.seq)
            if first or seq_id is not None: # we only want one
                break

    if len(feature_dfs) == 0:
        raise EmptyDataFrame("The annotation DataFrame is empty. Check that the feature_types and seq_id are correct, and that bounds (if specified) fall within the size of your genome.")
    return seqs, feature_dfs

# %% ../nbs/API/04_utils.ipynb 46
def parse_genbank(gb_path, # path to the genbank file
                  seq_id: Optional[str] = None, # sequence id (first column of the gff), if not None, then return only the annotations for the seq_id with this name
                  first = True, # if True then return only the annotations for the first sequence (or the first with seq_id)
                  bounds: Optional[tuple] = None, # (left limit, right limit)
                  feature_types: Optional[list] = None, # list of feature types to extract
                  attributes: Optional[Dict[str, List]] = None, # a dictionary with feature types as keys and a list of attributes to extract as values 
                  )->Tuple[List[Seq], List[pd.DataFrame]]:

    with open(gb_path,"r") as f:
        recs = parse_recs(SeqIO.parse(f, "genbank"), seq_id, first, bounds, feature_types, attributes)
    return recs


# %% ../nbs/API/04_utils.ipynb 49
def inspect_feature_types(file_path: str, 
                          frmt: str #gff or genbank
                          ):
    """Outputs a table that recapitulates the feature types and attributes available in the file."""
    
    if frmt == "genbank":
        _, dfs=parse_genbank(file_path)
    elif frmt == "gff":
        dfs=parse_gff(file_path)

    table_data=[]
    for df in dfs:
        for t in set(df.type):
            row=[t]
            attributes = df.loc[df.type==t, "attributes"].iloc[0]
            for attr in attributes:
                row.append(attr)
                table_data.append(row)
                row=[""]


    df_output = pd.DataFrame(table_data, columns=["feature_type", "attributes"])
    display(HTML(df_output.to_html(index=False)))

# %% ../nbs/API/04_utils.ipynb 54
def in_wsl() -> bool:
    return 'microsoft-standard' in uname().release

# %% ../nbs/API/04_utils.ipynb 56
def add_extension(filename,extension="svg"):
    base_name, ext = os.path.splitext(filename)
    if ext.lower() != '.'+extension:
        filename += '.'+extension
    return filename

# %% ../nbs/API/04_utils.ipynb 60
from bokeh.plotting import show as bk_show
from bokeh.layouts import column, row
from bokeh.io import output_notebook, reset_output
from bokeh.plotting import save as bk_save #Need to rename the bokeh show function so that there is no confusion with GenomeBrowser.show
from bokeh.plotting import output_file as bk_output_file #Need to rename the bokeh show function so that there is no confusion with GenomeBrowser.show
from bokeh.io import output_notebook, reset_output, export_png, export_svgs, export_svg
from svgutils import compose
import os
import warnings
from selenium.webdriver.chrome.options import Options
from selenium import webdriver

# %% ../nbs/API/04_utils.ipynb 61
def _save(elements, heights, width, fname:str, title:str="Genome Plot"):
    base_name, ext = os.path.splitext(fname)
    ext = ext.lower()
    if ext not in {".svg", ".png"}:
        raise ValueError(f"filename must end in svg or png, not {ext}")
    
    reset_output()
    bk_output_file(filename=fname, title=title)
    
    layout = column(elements)

    if in_wsl():
            ## Setup chrome options
            chrome_options = Options()
            chrome_options.add_argument("--headless") # Ensure GUI is off
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-3d-apis")
            chrome_options.add_argument("--disable-blink-features")
            

            homedir = os.path.expanduser("~")
            try:
                    # webdriver_service = Service(f"{homedir}/chromedriver/stable/chromedriver")
                    # browser = webdriver.Chrome(service=webdriver_service, options=chrome_options)
                    browser = webdriver.Chrome(options=chrome_options)
            except:
                    warnings.warn("""If using WSL you can install chromedriver following these instructions:https://scottspence.com/posts/use-chrome-in-ubuntu-wsl
                                  Also make sure the chromedriver-binary python package has the same major version number as your chrome install.
                                  Check the chrome version using: google-chrome --version
                                   Then use pip to force install of a web driver with a compatible version, for example:
                                   pip install --force-reinstall -v "chromedriver-binary==121.0.6167.184.0"
                                   """)
                    browser=None

            
    else:
            browser=None

    if ext == ".svg":
        #export_svg(layout, filename=fname)
        export_svgs(layout, filename=fname, webdriver=browser)
        if len(heights)>1: # TODO: what is this?
            total_height=sum(heights)
            svgelements=[compose.SVG(fname)]
            offset=heights[0]
            for i, height in enumerate(heights[1:]):
                svgelements.append(
                    compose.SVG(f"{base_name}_{i+1}.svg").move(0,offset)
                )
                offset+=height
                
            compose.Figure(width+50, # +50 accounts for axis and labels
                           total_height, 
                           *svgelements).save(f"{base_name}_composite.svg")

    else:
        export_png(layout, filename=fname, webdriver=browser)
    
    reset_output()

# %% ../nbs/API/04_utils.ipynb 65
def _save_html(elements, fname:str, title:str):
    reset_output()
    bk_output_file(filename=fname, title=title, mode='inline')
    bk_save(column(elements))
    reset_output()

# %% ../nbs/API/04_utils.ipynb 66
def _gb_show(elements):
    reset_output()
    output_notebook(hide_banner=True)
    bk_show(column(elements))
    reset_output()
